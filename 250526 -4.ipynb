{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d83bc56",
   "metadata": {},
   "source": [
    "LangChainMemory\n",
    "```\n",
    "LangChainMemory : 이전 대화 내용을 기억해서 문맥을 유지하는 역할 LangChain 0.3X부터는 LCEL 기반으로 체인을 구성,\n",
    "RunnableWithMessageHistoru, ChatMessageHistory 등의 컴퍼넌트를 활용해서 세션별 대화 기록을 관리, 대화가 장기화될 경우 요약 메모리를 도입해서 과거 대화를 LLM으로 요약하고 축약된 형태로 저장해서 프롬프트의 길이 문제를 해결결\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c355362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3040fef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dotenv.main.load_dotenv(dotenv_path: Union[str, ForwardRef('os.PathLike[str]'), NoneType] = None, stream: Optional[IO[str]] = None, verbose: bool = False, override: bool = False, interpolate: bool = True, encoding: Optional[str] = 'utf-8') -> bool>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4647d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human : 언녕하세요 제 이름은 이주영영입니다.\n",
      "ai : 안녕하세요 이주영님, 무엇을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "# 메모리 객체 생성\n",
    "history = InMemoryChatMessageHistory()\n",
    "history.add_user_message('언녕하세요 제 이름은 이주영영입니다.')\n",
    "history.add_ai_message('안녕하세요 이주영님, 무엇을 도와드릴까요?')\n",
    "# 현재까지의 대화 내용 확인\n",
    "for msg in history.messages:\n",
    "    print(f'{msg.type} : {msg.content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c933f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_redis'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Radis 기반 채팅 기록 저장소\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_redis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RedisChatMessageHistory\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m REDIS_URL = os.getenv(\u001b[33m'\u001b[39m\u001b[33mREDIS_URL\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mredis://localhost:6379\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_redis'"
     ]
    }
   ],
   "source": [
    "# Radis 기반 채팅 기록 저장소\n",
    "\n",
    "from langchain_redis import RedisChatMessageHistory\n",
    "import os\n",
    "REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379')\n",
    "session_id = 'user_123'\n",
    "\n",
    "history = RedisChatMessageHistory(session_id = session_id, redis_url = REDIS_URL)\n",
    "history.add_user_message('안녕하세요 제 이름은 홍길동 입니다.')\n",
    "history.add_ai_message('안녕하세요 홍길동님, 무엇을 도와드릴까요?')\n",
    "\n",
    "# 현재까지의 대화 내용 확인\n",
    "for msg in history.messages:\n",
    "    print(f'{msg.type} : {msg.content}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6b303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션기반 다중사용자 메모리 구조 구현 - 다중사용자 챗복\n",
    "# 핵심: session_id를 키로 하는 메모리 저장소 만들고 사용자의 대화는 키별로 저장한다\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "# 프롬프트\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",'당신은 뛰어난 한국어 상담 챗봇입니다. 질문에 친절하고 자세히 답변해주세요'),\n",
    "    #history 키로 전달된 메세지 목록은 chain 실행시 해당 위치에 넣겠다는 의미\n",
    "    MessagesPlaceholder(variable_name='history'), \n",
    "    ('human','{input}')\n",
    "])\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "322df069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3694e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션별 메모리 저장소를 딕셔너리로 만들고, 존재하지 않는 새로운 세션 id가 들어오면 InMemoryChatMessageHistory를 생성\n",
    "# get_sessino_history 를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aea5995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "#세션 id -> 대화 기록 객체 매핑\n",
    "store = {}\n",
    "def get_session_history(session_id : str) -> InMemoryChatMessageHistory:\n",
    "    '''\"세션 ID\"에 해당하는 대화 기록 객체를 반환합니다. (없으면 새로 생성)'''\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 메모리를 통합한 체인 래퍼생성\n",
    "chatbot = RunnableWithMessageHistory(\n",
    "    runnable = chain,\n",
    "    get_session_history = get_session_history,\n",
    "    input_messages_key= 'input',\n",
    "    history_messages_key= 'history'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0a295f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[user_a] 질문: 안녕하세요, 저는 홍길동입니다. 당신은 누구신가요?\n",
      "[user_a] 챗봇: 안녕하세요, 홍길동님! 저는 여러분의 질문에 답변하고 도움을 드리기 위해 만들어진 챗봇입니다. 어떤 궁금한 점이나 도움이 필요하신 부분이 있으신가요?\n",
      "\n",
      "[user_b] 질문: 안녕하세요, 저는 이순신입니다. 당신은 어떤 일을 하시나요?\n",
      "[user_b] 챗봇: 안녕하세요, 이순신님! 저는 여러분의 질문에 답변하고, 다양한 정보와 도움을 제공하는 챗봇입니다. 궁금한 점이나 도움이 필요한 부분이 있다면 언제든지 말씀해 주세요!\n",
      "\n",
      "[user_a] 질문: 저는 프로그래밍을 배우고 있습니다. 당신은 어떤 일을 하시나요?\n",
      "[user_a] 챗봇: 프로그래밍을 배우고 계시다니 멋지네요! 저는 여러분의 질문에 답변하고, 정보 제공, 문제 해결, 그리고 다양한 주제에 대한 상담을 하는 역할을 하고 있습니다. 프로그래밍에 관련된 질문이나 도움이 필요하시면 언제든지 말씀해 주세요! 어떤 언어를 배우고 계신가요?\n",
      "\n",
      "[user_b] 질문: 저는 역사에 관심이 많습니다. 당신은 어떤 분야에 관심이 있나요?\n",
      "[user_b] 챗봇: 역사에 관심이 많으시다니 정말 멋지네요! 저는 특정한 관심 분야가 없지만, 다양한 주제에 대한 정보를 제공할 수 있습니다. 역사, 과학, 기술, 문화, 예술 등 여러 분야에 대해 이야기할 수 있으니, 궁금한 점이나 더 알고 싶은 주제가 있다면 말씀해 주세요!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 두개의 세션을 번갈아가면서 대화 RunnableWithMessageHistory 가 각 세션에 맞는 대화 기록을 관리합니다\n",
    "sessions = ['user_a','user_b']\n",
    "questions = [\n",
    "    '안녕하세요, 저는 홍길동입니다. 당신은 누구신가요?', #user_a 첫번째 질문\n",
    "    '안녕하세요, 저는 이순신입니다. 당신은 어떤 일을 하시나요?', #user_b 첫 번째 질문\n",
    "    '저는 프로그래밍을 배우고 있습니다. 당신은 어떤 일을 하시나요?', #user_a 두 번째 질문\n",
    "    '저는 역사에 관심이 많습니다. 당신은 어떤 분야에 관심이 있나요?' #user_b 두 번째 질문\n",
    "]\n",
    "for i,question in enumerate(questions):\n",
    "    session_id = sessions[i%2] # 세션 ID를 번갈아가며 사용 \n",
    "    result = chatbot.invoke({'input':question}, config = {'configurable': {'session_id':session_id}})\n",
    "    print(f'[{session_id}] 질문: {question}')\n",
    "    print(f'[{session_id}] 챗봇: {result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e58a4757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[user_c] 질문: 저는 철수예요, 반갑습니다.\n",
      "[user_c] 챗봇: 안녕하세요, 철수님! 반갑습니다. 어떻게 도와드릴까요? 궁금한 점이나 이야기하고 싶은 것이 있다면 말씀해 주세요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = chatbot.invoke({'input':\"저는 철수예요, 반갑습니다\"}, config={'configurable' : {'session_id' : 'user_c'}})\n",
    "print(f'[user_c] 질문: 저는 철수예요, 반갑습니다.')\n",
    "print(f'[user_c] 챗봇: {result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "659ffc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'철수님이라고 하셨습니다! 맞나요? 더 궁금한 점이나 이야기하고 싶은 것이 있다면 언제든지 말씀해 주세요.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chatbot.invoke({'input' : '저는 누구라고요?'}, config = {'configurable': {'session_id' : 'user_c'}})\n",
    "result\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c95a3",
   "metadata": {},
   "source": [
    "요약 메모리 구현(대화내용 자동 요약)\n",
    "```\n",
    "긴 대화내용을 모두 프롬프트에 기록하는 것은 비 효율적 -> 프롬프트에 길이 제한에 걸릴 가능성이 있음\n",
    "Conversation Summary Memory\n",
    "0.3x 버전에서는 직접 요약용 체인을 만들어서 ChatMessageHistory에 적용\n",
    "```\n",
    "어떻게 요약?\n",
    "```\n",
    "- 일정길이 이상으로 대화가 누적되면, 과거 대화를 요약해서 핵심내용만 남김\n",
    "- 요약결과를 메모리에 시스템 메시지 등으로 저장 -> 메모리 절약\n",
    "- 새로운 사용자 입력 시 요약된 맥락 + 최근 몇 메시지만 참고해서 llm 전달\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85bf8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약용 프롬프트 템플릿\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system','당신은 대화 요약 전문가입니다. 대화의 주요 내용을 간결하게 요약해 주세요'),\n",
    "    ('human','{conversation}')   # 잔체 대화 내용을 하나의 문자열로 전달\n",
    "])\n",
    "# LCEL  \n",
    "summary_chain = summary_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57ecb739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약전 user_d의 메모리 메세지 개수: 10\n",
      "Human: 안녕, 오늘 우리 뭐하려고 했지?\n",
      "AI: 안녕하세요! 오늘 어떤 이야기를 나누고 싶으신가요? 궁금한 점이나 하고 싶은 주제가 있다면 말씀해 주세요. 함께 이야기해보아요!\n",
      "Human: 아 맞다 내일 회의자료 준비해야지, 회의는 몇시지?\n",
      "AI: 회의 시간이 정확히 몇 시인지 기억이 나지 않으신가요? 회의 일정은 보통 이메일이나 캘린더에 기록되어 있을 텐데, 확인해보시면 좋을 것 같아요. 만약 회의 준비에 도움이 필요하시다면 어떤 자료를 준비해야 하는지 말씀해 주시면 도와드릴 수 있습니다!\n",
      "Human: 그 회의에 누가 참석하는지 기억나니?\n",
      "AI: 회의에 참석하는 사람들을 기억하는 것은 중요하지만, 제가 그 정보를 알 수는 없어요. 보통 회의 초대장이나 이메일에 참석자 목록이 포함되어 있을 텐데, 그걸 확인해보시면 좋을 것 같아요. 참석자 목록을 알고 계신다면, 그들과의 논의 주제나 역할에 대해 이야기해드릴 수 있습니다! 도움이 필요하시면 말씀해 주세요.\n",
      "Human: 단위 프로젝트 진행 상황도 공유해야 할까?\n",
      "AI: 네, 단위 프로젝트 진행 상황을 공유하는 것은 좋은 아이디어입니다. 회의에서 프로젝트의 현재 상태, 진행 중인 작업, 완료된 작업, 그리고 앞으로의 계획 등을 공유하면 참석자들이 프로젝트에 대한 이해를 높이고, 필요한 피드백이나 지원을 받을 수 있습니다. \n",
      "\n",
      "또한, 진행 상황을 공유함으로써 팀원 간의 협업을 촉진하고, 문제를 조기에 발견하여 해결할 수 있는 기회를 제공할 수 있습니다. 어떤 내용을 포함할지 고민 중이라면, 구체적인 진행 상황이나 이슈, 다음 단계 등을 정리해보세요. 도움이 필요하시면 언제든지 말씀해 주세요!\n",
      "Human: 최근에 이야기 했던 새로운 기능에 대한 업데이트는 있어?\n",
      "AI: 새로운 기능에 대한 업데이트는 프로젝트의 진행 상황에 따라 다를 수 있습니다. 만약 그 기능에 대한 구체적인 내용이나 개발 상황을 알고 계신다면, 그 정보를 바탕으로 업데이트를 준비하시면 좋을 것 같습니다. \n",
      "\n",
      "업데이트를 공유할 때는 다음과 같은 내용을 포함하면 유용할 수 있습니다:\n",
      "\n",
      "1. **기능 설명**: 새로운 기능이 무엇인지 간단히 설명합니다.\n",
      "2. **진행 상황**: 현재 개발이 어느 단계에 있는지, 완료된 부분과 진행 중인 부분을 나누어 설명합니다.\n",
      "3. **예상 일정**: 기능이 언제 완료될 것으로 예상되는지, 배포 일정은 어떻게 되는지 공유합니다.\n",
      "4. **이슈 및 해결 방안**: 개발 중 발생한 문제나 이슈가 있다면, 그에 대한 해결 방안도 함께 공유합니다.\n",
      "\n",
      "이런 정보를 준비하면 회의에서 유익한 논의가 이루어질 수 있을 것입니다. 추가적인 도움이 필요하시면 언제든지 말씀해 주세요!\n"
     ]
    }
   ],
   "source": [
    "# user_d 세션에 대화내용을 기록 간 대화 생성\n",
    "long_queries = [\n",
    "    '안녕, 오늘 우리 뭐하려고 했지?',\n",
    "    '아 맞다 내일 회의자료 준비해야지, 회의는 몇시지?',\n",
    "    '그 회의에 누가 참석하는지 기억나니?',\n",
    "    '단위 프로젝트 진행 상황도 공유해야 할까?',\n",
    "    '최근에 이야기 했던 새로운 기능에 대한 업데이트는 있어?'\n",
    "]\n",
    "session_id = 'user_d'\n",
    "for q in long_queries:\n",
    "    answer = chatbot.invoke({'input':q}, config={'configurable' : {'session_id' : session_id}})\n",
    "\n",
    "print(f'요약전 user_d의 메모리 메세지 개수: {len(store[session_id].messages)}')\n",
    "print(store[session_id]) #요약하기전 대화 내용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8b8ab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== 요약 내용 ==\n",
      "대화 요약: \n",
      "사용자는 내일 회의 자료를 준비해야 한다고 언급하며 회의 시간과 참석자를 확인하려고 한다. AI는 회의 일정과 참석자 목록을 이메일이나 캘린더에서 확인할 것을 제안하고, 단위 프로젝트 진행 상황을 회의에서 공유하는 것이 좋다고 조언한다. AI는 진행 상황 공유가 팀 협업과 문제 해결에 도움이 될 것이라고 강조하며, 필요한 경우 추가 도움을 제공할 준비가 되어 있다고 말한다.\n"
     ]
    }
   ],
   "source": [
    "# 전체 대화 내용을 요약하고 마지막 사용자 질문-답변 쌍만 원본 유지\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "# 요약 대상 대화 내용 추출(마지막 QA 쌍 제외한 이전 내용)\n",
    "message = store[session_id].messages\n",
    "\n",
    "# 1번 주고받은 대화 이후에 또 했다는 것 -> 마지막을 빼야함\n",
    "if len(message) > 2 :\n",
    "    original_dialog = '\\n'.join([f'{msg.type.upper()} : {msg.content}' for msg in message[ : -2]])\n",
    "else : \n",
    "    original_dialog = '\\n'.join([f'{msg.type.upper()} : {msg.content}' for msg in message])\n",
    "\n",
    "# llm으로 요약 생성\n",
    "summary_text = summary_chain.invoke({'conversation' : original_dialog})\n",
    "print(\"== 요약 내용 ==\")\n",
    "print(summary_text)\n",
    "# 기존 메모리를 요약으로 교체 : 이전 내용 요약본 + 최근 QA 유지\n",
    "new_history = InMemoryChatMessageHistory()\n",
    "new_history.messages.append(SystemMessage(content = f'요약: {summary_text}'))\n",
    "\n",
    "# 최근 대화의 마지막 QA쌍 복원\n",
    "if len(message) >= 2 :\n",
    "    last_user_msg = message[-2]\n",
    "    last_ai_msg = message[-1]\n",
    "    # 휴먼 메시지인지 검사\n",
    "    if isinstance(last_user_msg, HumanMessage) :\n",
    "        new_history.add_user_message(last_user_msg.content)\n",
    "    else :\n",
    "        new_history.messages.append(last_user_msg)\n",
    "    if isinstance(last_ai_msg, AIMessage) :\n",
    "        new_history.add_ai_message(last_ai_msg.content)\n",
    "    else :\n",
    "        new_history.messages.append(last_ai_msg)\n",
    "# 메모리 교체\n",
    "store[session_id] = new_history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61e357",
   "metadata": {},
   "source": [
    "최신 체인 구성 방법 v0.3\n",
    "```\n",
    "LLMChain, SequentialChain 등과 같이 클래스의존도를 줄임\n",
    "Runnable 공통 인터페이스를 통해 일관성을 유지\n",
    "핵심 : Runnable + Composition --> 프롬프트 | 모델 | 파서\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2021cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: openai in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (1.77.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain) (0.3.61)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\miniconda3\\envs\\llm.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af3b937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-pr****\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # .env파일의 내용을 읽어서 환경변수에 등록\n",
    "import os\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "print(openai_key[:5]+'****')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0188fb2",
   "metadata": {},
   "source": [
    "단일 체인 : prompt -> llm -> 출력파서 (상품설명)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0144b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제품 이름 : 선풍기\n",
      "이 제품의 특징과 장점을 잘 표현할 수 있는 한개의 문장으로 변동해 주세요\n",
      "\"강력한 바람과 조용한 작동으로 여름철 더위를 시원하게 날려주는 선풍기입니다.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. 프롬프트 템플릿을 정의\n",
    "product_prompt = PromptTemplate.from_template(\n",
    "    '제품 이름 : {product}\\n'\n",
    "    '이 제품의 특징과 장점을 잘 표현할 수 있는 한개의 문장으로 변동해 주세요'\n",
    ")\n",
    "print(product_prompt.format(product = '선풍기'))\n",
    "# 2. 출력파서 정의\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 3. llm 정의\n",
    "llm = ChatOpenAI(model = 'gpt-4o-mini', temperature=0)\n",
    "\n",
    "# 4. LCEL 체인 구성 프롬프트 -> 모델 -> 출력파서\n",
    "product_chain = product_prompt | llm | output_parser  # runnable 객체\n",
    "\n",
    "result = product_chain.invoke({'product' : '선풍기'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae4c774",
   "metadata": {},
   "outputs": [],
   "source": [
    " # # OpenAI는 Text Completion, 즉 문장을 이어서 완성하는 모델...\n",
    "# # text-davinci 모델을 위한 라이브러리 이전방식이라, 역할기반 템플릿이 없고 단순한 prompt\n",
    "\n",
    "# from langchain_openai import OpenAI\n",
    "# llm = OpenAI(model='text-davinci-003', temperature=0.7)\n",
    "# print(llm.invoke('다음 문장을 완성하세요: 옛날 옛적에 호랑이가 살았습니다.'))\n",
    "\n",
    "# text-davinci는 작년 7월에 사라짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847263a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 그 호랑이는 사람을 잡아먹는 무서운 호랑이였습니다. 그래서 사람들은 호랑이를 피하기 위해서 ��에 들어가지 않았습니다. 그런데 어느 날, 호랑이는 배가 너무 고파져서 ��속으로 나가 사람을 잡아먹기로 결심했습니다. 호랑이는 ��속으로 들어가면서, \"이제 나는 나의 맛있는 저녁을 찾을 거야!\"라고 생각했습니다.\n",
      "\n",
      "호랑이는 ��속 깊은 곳으로 들어가면서 여러 가지 동물들을 만났습니다. 사슴, 토끼, 그리고 다람���들이었습니다. 하지만 호랑이는 그들을 잡아먹지 않았습니다. 왜냐하면 그들은 너무 귀��고 무서워 보이지 않았기 때문입니다. 호랑이는 그 대신에 더 큰 먹이를 찾아 계속 ��속을 돌아다��습니다. \n",
      "\n",
      "그러다 호랑이는 한 마을을 발견했습니다. 호랑이는 그 마을로 들어가서 사람들을 찾아봤습니다. 그런데 마을 사람들은 호랑이를 보고 너무 놀라서 도망가버렸습니다. 호랑이는 \"이렇게 쉽게 사람들을 잡을 수 있다니!\"라며\n"
     ]
    }
   ],
   "source": [
    "# OpenAI는 Text complation 문장을 이어서 완성하는 모델...\n",
    "# text-davinci 모델을 위한 라이브러리 이전방식이라서, 역할기반 템플릿이 없고 단순한 prompt\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model='gpt-4o-mini',temperature=0.7)\n",
    "print(llm.invoke('다음 문장을 완성히세요 : 옛날옛적에 호랑이가 살았습니다'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41f12a",
   "metadata": {},
   "source": [
    "다중체인 : 체인 합성 및 Runnable 병합 (이메일 생성)\n",
    "```\n",
    "둘 이상의 llm 호출을 연결해서 복잡한 작업을 수행\n",
    "'주어진 상황에 대한 이메일 작성' --> 제목\n",
    "제목을 활용해서 이메일 본문을 작성 --> 본문\n",
    "chain composition\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af82cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 1 : 사용자로부터 받은 이메일 요청 내용을 입력받아서 '이메일 제목'을 한문장으로 생성하는 명령\n",
    "# llm 호출 -> 이메일 제목 출력('프로젝트 진행상황 회의 일정 안내')\n",
    "# 중간 출력 변환 : 생성된 제목문자열을 {subject} 키를 갖는 dictionary 변환\n",
    "# 프롬프트 2 : {subject} 변수를 받아서 해당 제목을 가진 이메일 본문내용을 요청\n",
    "# llm 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e19a814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  다음주 단위 프로젝트 팀 회의 요청\n",
      "\n",
      "안녕하세요 팀원 여러분,\n",
      "\n",
      "다음 주 단위 프로젝트를 위한 팀 회의를 요청드립니다. 프로젝트의 진행 상황을 점검하고, 각자의 역할을 정리하여 보다 효율적으로 작업을 이어나가기 위해 꼭 필요한 시간이 될 것입니다.\n",
      "\n",
      "회의에서 다�� 주요 내용은 다음과 같습니다:\n",
      "1. 프로젝트 목표 및 일정 확인\n",
      "2. 역할 분담 및 책임 확인\n",
      "3. 진행 상황 점검 및 피드백 공유\n",
      "\n",
      "모든 팀원 여러분의 소중한 의견이 필요합니다. 아래의 가능한 일정을 알려주시면 감사하겠습니다.\n",
      "\n",
      "- 날짜 및 시간:\n",
      "- 기타 제안 사항:\n",
      "\n",
      "여러분의 협조에 감사드리며, 회의에서 ���기를 기대합니다.\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "[당신의 이름]  \n",
      "[당신의 직책]  \n",
      "[당신의 연락처]  \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# 1. 이메일 생성용 프롬프트 정의\n",
    "subject_prompt = PromptTemplate.from_template(\n",
    "    '다음 요청 내용을 바탕으로 이메일 제목을 만들어주세요'\n",
    "    '{content}'\n",
    ")\n",
    "# 2. 이메일 본문 생성용 프롬프트\n",
    "body_prompt = PromptTemplate.from_template(\n",
    "    '위에서 생성된 제목을 활용해서 팀에게 보내는 정중한 이메일 본문을 작성해 주세요\\n'\n",
    "    '제목 : {subject}\\n'\n",
    "    '본문 :'\n",
    ")\n",
    "# 3. 두 프롬프트를 결합한 체인 구성\n",
    "email_chain = (\n",
    "    subject_prompt\n",
    "    | llm\n",
    "    | {'subject' : RunnablePassthrough()}  # 출력된 제목을 subject 키로 매핑\n",
    "    | body_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# 4. 실행\n",
    "result = email_chain.invoke({\n",
    "    'content' : '다음주 단위 프로젝트를 진행하기 위해 팀 회의를 요청하는 이메일'\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d277da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 다음주 단위 프로젝트 팀 회의 요청\n",
      "\n",
      "제목: 다음주 단위 프로젝트 팀 회의 요청\n",
      "\n",
      "안녕하세요, 팀 여러분.\n",
      "\n",
      "다음주에 진행할 단위 프로젝트에 대해 논의하기 위해 팀 회의를 요청드립니다. 모든 팀원들이 참석할 수 있는 최적의 일정을 조율하고자 합니다.\n",
      "\n",
      "가능하신 날짜와 시간을 댓글로 남겨주시면, 가장 많은 분들이 참석할 수 있는 시간으로 회의를 잡도록 하겠습니다.\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "[당신의 이름]  \n",
      "[당신의 직책]  \n",
      "[당신의 연락처]\n",
      "\n",
      "---\n",
      "\n",
      "이메일 내용이 적절한지 확인하시고, 필요에 따라 수정해 주세요! 추가로 요청하실 사항이 있으시면 언제든지 말씀해 주세요. 감사합니다!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# 1. 이메일 생성용 프롬프트 정의\n",
    "subject_prompt = PromptTemplate.from_template(\n",
    "    '다음 요청 내용을 바탕으로 이메일 제목을 만들어주세요\\n'\n",
    "    '{content}'\n",
    ")\n",
    "# 2. 이메일 본문 생성용 프롬프트\n",
    "body_prompt = PromptTemplate.from_template(\n",
    "    '위에서 생성된 제목을 활용해서 팀에게 보내는 정중한 이메일 본문을 작성해 주세요\\n'\n",
    "    '제목:{subject}\\n'\n",
    "    '본문:'\n",
    ")\n",
    "# 3. 두 프롬프트를 결합한 체인을 구성\n",
    "email_chain = (\n",
    "    subject_prompt\n",
    "    | llm\n",
    "    | {'subject' : RunnablePassthrough()} # 출력된 제목을 subject 키로 매핑\n",
    "    | body_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# 4. 실행\n",
    "result = email_chain.invoke({\n",
    "    'content' : '다음주 단위 프로젝트를 진행하기 위해 팀 회의를 요청하는 이메일'\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83cefd",
   "metadata": {},
   "source": [
    "조건분기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59e6d1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result1:  참석자들은 변경 사항에 대한 의견을 나누었고, 예산 조정의 필요성에 대해 공감했습니다.\n",
      "\n",
      "오늘 회의에서는 일정 변경과 예산 조정에 대한 의견이 활발히 논의되었습니다. 참석자들은 이 변경 사항에 대해 의견을 교환하며 예산 조정의 필요성에 공감했습니다.\n",
      "result2:  \n",
      "안녕하세요 팀원 여러분,\n",
      "\n",
      "다음주 월요일에 예정된 프로젝트 회의에 대한 일정을 안내드립니다. 이번 회의에서는 현재 진행 중인 프로젝트의 진행 상황을 점검하고, 향후 계획에 대해 논의할 예정입니다.\n",
      "\n",
      "회의는 다음과 같은 일정으로 진행됩니다:\n",
      "- 일시: 다음주 월요일, 오후 2시\n",
      "- 장소: 3층 회의실 A\n",
      "\n",
      "회의에 참석하실 수 있는 모든 분들의 참여를 부탁드립니다. 회의 준비를 위해 각자 맡은 부분의 진행 상황을 정리해오시면 감사하겠습니다.\n",
      "\n",
      "궁금한 사항이 있으시면 언제든지 저에게 문의해 주세요.\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "[당신의 이름]  \n",
      "[당신의 직함]  \n",
      "[회사 이름]  \n",
      "[연락처]  \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "# 1. 요약체인(prompt->llm)\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    '다음 문장을 한 문단으로 간결하게 요약해주세요\\n'\n",
    "    '{text}'\n",
    ")\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "# 2. email chain은 재활용\n",
    "# 3. 분기조건 함수 정의 -> Runnable로 래핑\n",
    "def is_summary_request(user_input:str) -> bool:\n",
    "    return user_input.strip().startswith('요약:')\n",
    "condition = RunnableLambda(is_summary_request)\n",
    "# 분기 체인\n",
    "branch_chain = RunnableBranch(\n",
    "    (condition, summary_chain), email_chain\n",
    ")\n",
    "# 다양한 조건\n",
    "input1 = '요약: 오늘 회의에서는 다양한 주제에 대한 토론이 있었는데, 특히 일정 변경과 예산 관련 내용이 다수였습니다.'\n",
    "input2 = '이메일: 다음주 월요일 프로젝트 회의 일정을 팀에게 공지해줘'\n",
    "\n",
    "result1 = branch_chain.invoke(input1)\n",
    "result2 = branch_chain.invoke(input2)\n",
    "print(f'result1: {result1}')\n",
    "print(f'result2: {result2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614cb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
